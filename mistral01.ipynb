{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bce953-7b17-4d4f-92d9-c57ad855b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install groq\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f93b5dd7-508e-4695-9083-f322436ac2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low latency Large Language Models (LLMs) are important in certain applications due to their ability to process and respond to inputs quickly. Latency refers to the time delay between a user's request and the system's response. In some real-time or time-sensitive applications, low latency is crucial for providing a good user experience and ensuring that the system can respond to changing conditions in a timely manner.\n",
      "\n",
      "For example, in conversational agents or chatbots, low latency is important for maintaining the illusion of a real-time conversation. If there is a significant delay between the user's input and the agent's response, it can disrupt the flow of the conversation and make it feel less natural. Similarly, in applications such as online gaming or financial trading, low latency is critical for enabling users to make decisions and take actions quickly based on real-time data.\n",
      "\n",
      "Moreover, low latency LLMs can help reduce the computational cost of running large language models. By reducing the amount of time that the model spends processing each input, it is possible to serve more requests within a given amount of time, or to use fewer resources to serve the same number of requests. This can make large language models more practical and cost-effective to deploy in real-world applications.\n",
      "\n",
      "Overall, low latency LLMs are important for enabling real-time or time-sensitive applications to provide a good user experience, make decisions quickly, and reduce computational cost.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of low latency LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7e398-fb94-4a5a-8b0a-fd7233cde6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
